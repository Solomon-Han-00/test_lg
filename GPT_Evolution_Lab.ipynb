{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”¬ GPT ì‹œë¦¬ì¦ˆì˜ ì§„í™”: GPT-1 â†’ GPT-2 â†’ GPT-3\n",
    "# Architectural Evolution of the GPT Series\n",
    "\n",
    "---\n",
    "\n",
    "## í•™ìŠµ ëª©í‘œ (Learning Objectives)\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ ì™„ë£Œí•˜ë©´ ë‹¤ìŒì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. GPT-1, GPT-2, GPT-3 ì‚¬ì´ì˜ **êµ¬ì²´ì ì¸ ì•„í‚¤í…ì²˜ ì°¨ì´ì **ì„ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤\n",
    "2. **Pre-Norm vs Post-Norm** LayerNorm ë°°ì¹˜ì˜ ì°¨ì´ì™€ ê·¸ íš¨ê³¼ë¥¼ ì´í•´í•œë‹¤\n",
    "3. **Weight initialization scaling** ($1/\\sqrt{N}$)ì˜ í•„ìš”ì„±ê³¼ ì›ë¦¬ë¥¼ ì´í•´í•œë‹¤\n",
    "4. GPT-3ì˜ **Sparse Attention** íŒ¨í„´ì„ ì´í•´í•˜ê³  dense attentionê³¼ ë¹„êµí•  ìˆ˜ ìˆë‹¤\n",
    "5. **Zero-shot â†’ Few-shot â†’ In-context learning**ìœ¼ë¡œì˜ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì„ ì‹¤í—˜ì ìœ¼ë¡œ í™•ì¸í•œë‹¤\n",
    "6. HuggingFaceë¥¼ í™œìš©í•˜ì—¬ GPT ëª¨ë¸ì˜ ë‚´ë¶€ êµ¬ì¡°ë¥¼ ì§ì ‘ íƒìƒ‰í•  ìˆ˜ ìˆë‹¤\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š í•µì‹¬ ë…¼ë¬¸ ë ˆí¼ëŸ°ìŠ¤ (Key Paper References)\n",
    "\n",
    "| Model | Paper | Authors | Year |\n",
    "|-------|-------|---------|------|\n",
    "| GPT-1 | *Improving Language Understanding by Generative Pre-Training* | Radford et al. | 2018 |\n",
    "| GPT-2 | *Language Models are Unsupervised Multitask Learners* | Radford et al. | 2019 |\n",
    "| GPT-3 | *Language Models are Few-Shot Learners* | Brown et al. | 2020 |\n",
    "| Transformer | *Attention Is All You Need* | Vaswani et al. | 2017 |\n",
    "| Sparse Transformer | *Generating Long Sequences with Sparse Transformers* | Child et al. | 2019 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 0: í™˜ê²½ ì„¤ì • (Environment Setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install -q transformers torch matplotlib numpy pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel, GPT2Tokenizer, GPT2Config,\n",
    "    OpenAIGPTLMHeadModel, OpenAIGPTTokenizer, OpenAIGPTConfig,\n",
    "    pipeline\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì‹œê°í™” ìŠ¤íƒ€ì¼\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: GPT ì‹œë¦¬ì¦ˆ ì „ì²´ ê°œìš” (Overview)\n",
    "\n",
    "GPT(Generative Pre-trained Transformer) ì‹œë¦¬ì¦ˆëŠ” Vaswani et al. (2017)ì˜ Transformer ì•„í‚¤í…ì²˜ ì¤‘ **Decoder ë¶€ë¶„**ë§Œì„ ì‚¬ìš©í•˜ëŠ” autoregressive language modelì…ë‹ˆë‹¤.\n",
    "\n",
    "ì„¸ ëª¨ë¸ ëª¨ë‘ ê¸°ë³¸ì ìœ¼ë¡œ ê°™ì€ ì›ë¦¬ë¥¼ ê³µìœ í•©ë‹ˆë‹¤:\n",
    "- **Decoder-only** Transformer\n",
    "- **Causal (masked) self-attention**: ê° í† í°ì€ ì´ì „ í† í°ë“¤ë§Œ ì°¸ì¡°\n",
    "- **Next-token prediction** ëª©ì í•¨ìˆ˜\n",
    "- **Learned positional embeddings** (sinusoidalì´ ì•„ë‹Œ í•™ìŠµëœ ìœ„ì¹˜ ì„ë² ë”©)\n",
    "\n",
    "ê·¸ëŸ¬ë‚˜ ì„¸ëŒ€ë¥¼ ê±°ë“­í•˜ë©´ì„œ **\"ë‹¨ìˆœíˆ í¬ê¸°ë§Œ í‚¤ìš´ ê²ƒ\"ì´ ì•„ë‹Œ** ì¤‘ìš”í•œ ì•„í‚¤í…ì²˜ ë³€ê²½ê³¼ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì´ ìˆì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT ì‹œë¦¬ì¦ˆ í•µì‹¬ ìŠ¤í™ ë¹„êµí‘œ\n",
    "comparison_data = {\n",
    "    'í•­ëª©': [\n",
    "        'Parameters', 'Layers', 'd_model', 'Attention Heads',\n",
    "        'd_head', 'd_ff', 'Context Length', 'Vocab Size',\n",
    "        'Training Data', 'Training Data Size',\n",
    "        'Training Paradigm', 'LayerNorm Position',\n",
    "        'Activation Function', 'Attention Type'\n",
    "    ],\n",
    "    'GPT-1 (2018)': [\n",
    "        '117M', '12', '768', '12',\n",
    "        '64', '3072', '512', '40,478',\n",
    "        'BooksCorpus', '~5GB (ì•½ 10ì–µ ë‹¨ì–´)',\n",
    "        'Pre-train + Fine-tune', 'Post-Norm (ê¸°ì¡´ Transformer ë°©ì‹)',\n",
    "        'GELU', 'Dense (Full)'\n",
    "    ],\n",
    "    'GPT-2 (2019)': [\n",
    "        '1.5B (ìµœëŒ€)', '48 (ìµœëŒ€)', '1600 (ìµœëŒ€)', '25 (ìµœëŒ€)',\n",
    "        '64', '6400 (ìµœëŒ€)', '1024', '50,257',\n",
    "        'WebText', '~40GB (ì•½ 80ì–µ ë‹¨ì–´)',\n",
    "        'Zero-shot (Fine-tune ì—†ì´)', 'Pre-Norm âœ¨',\n",
    "        'GELU', 'Dense (Full)'\n",
    "    ],\n",
    "    'GPT-3 (2020)': [\n",
    "        '175B', '96', '12,288', '96',\n",
    "        '128', '49,152', '2048', '50,257',\n",
    "        'Common Crawl + WebText2 + Books + Wikipedia', '~570GB (ì•½ 3000ì–µ í† í°)',\n",
    "        'Few-shot / In-context Learning âœ¨', 'Pre-Norm',\n",
    "        'GELU', 'Alternating Dense + Sparse âœ¨'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(comparison_data)\n",
    "df = df.set_index('í•­ëª©')\n",
    "\n",
    "# ìŠ¤íƒ€ì¼ ì ìš©í•˜ì—¬ ì¶œë ¥\n",
    "def highlight_changes(val):\n",
    "    if 'âœ¨' in str(val):\n",
    "        return 'background-color: #fff3cd; font-weight: bold'\n",
    "    return ''\n",
    "\n",
    "styled_df = df.style.applymap(highlight_changes)\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **âœ¨ í‘œì‹œ**ëŠ” ì´ì „ ì„¸ëŒ€ ëŒ€ë¹„ ìƒˆë¡­ê²Œ ë„ì…ëœ í•µì‹¬ ë³€ê²½ì‚¬í•­ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n",
    ">\n",
    "> ë‹¨ìˆœíˆ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ëŠ˜ë¦° ê²ƒì´ ì•„ë‹ˆë¼, LayerNorm ìœ„ì¹˜, í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„, Attention êµ¬ì¡° ë“±ì—ì„œ\n",
    "> ì¤‘ìš”í•œ ì•„í‚¤í…ì²˜ í˜ì‹ ì´ ìˆì—ˆìŒì„ ì£¼ëª©í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: GPT-1 ì•„í‚¤í…ì²˜ ê¹Šì´ ë³´ê¸°\n",
    "\n",
    "### 2.1 GPT-1ì˜ í•µì‹¬ íŠ¹ì§•\n",
    "\n",
    "Radford et al. (2018)ì˜ GPT-1ì€ ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ì•„ì´ë””ì–´ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **Semi-supervised learning**: ë¹„ì§€ë„ ì‚¬ì „í•™ìŠµ(unsupervised pre-training) â†’ ì§€ë„ ë¯¸ì„¸ì¡°ì •(supervised fine-tuning)\n",
    "2. **Decoder-only Transformer**: Encoder-Decoder Attention ì œê±°, Masked Self-Attentionë§Œ ì‚¬ìš©\n",
    "3. **Task-specific input transformations**: ê° downstream taskì— ë§ëŠ” ì…ë ¥ ë³€í™˜ ë°©ì‹ ì •ì˜\n",
    "\n",
    "#### ì•„í‚¤í…ì²˜ ì„¸ë¶€ì‚¬í•­ (ì›ë¬¸ ì¸ìš©)\n",
    "\n",
    "> \"*We trained a 12-layer decoder-only transformer with masked self-attention heads\n",
    "> (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks,\n",
    "> we used 3072 dimensional inner states.*\"  \n",
    "> â€” Radford et al. (2018), \"Improving Language Understanding by Generative Pre-Training\"\n",
    "\n",
    "#### Post-Norm (ê¸°ì¡´ Transformer ë°©ì‹)\n",
    "\n",
    "GPT-1ì€ Vaswani et al. (2017)ì˜ ì›ë˜ Transformerì™€ ë™ì¼í•˜ê²Œ **Post-Norm**ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:\n",
    "\n",
    "$$\\text{Output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))$$\n",
    "\n",
    "ì¦‰, Residual connection ì´í›„ì— LayerNormì„ ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-1 ëª¨ë¸ ë¡œë“œ ë° êµ¬ì¡° í™•ì¸\n",
    "print(\"=\" * 60)\n",
    "print(\"GPT-1 ëª¨ë¸ ë¡œë“œ ì¤‘... (openai-community/openai-gpt)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "gpt1_config = OpenAIGPTConfig()\n",
    "print(\"\\nğŸ“‹ GPT-1 Configuration:\")\n",
    "print(f\"  - Vocab size: {gpt1_config.vocab_size}\")\n",
    "print(f\"  - Embedding dim (n_embd): {gpt1_config.n_embd}\")\n",
    "print(f\"  - Num layers (n_layer): {gpt1_config.n_layer}\")\n",
    "print(f\"  - Num heads (n_head): {gpt1_config.n_head}\")\n",
    "print(f\"  - Max position embeddings: {gpt1_config.n_positions}\")\n",
    "print(f\"  - Activation function: {gpt1_config.afn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-1 ëª¨ë¸ì˜ ì‹¤ì œ êµ¬ì¡° í™•ì¸\n",
    "gpt1_model = OpenAIGPTLMHeadModel.from_pretrained('openai-community/openai-gpt')\n",
    "\n",
    "print(\"\\nğŸ—ï¸ GPT-1 ì „ì²´ ì•„í‚¤í…ì²˜:\")\n",
    "print(gpt1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-1 íŒŒë¼ë¯¸í„° ìˆ˜ ìƒì„¸ ë¶„ì„\n",
    "def count_parameters_by_module(model, model_name):\n",
    "    \"\"\"ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  í‘œë¡œ ì¶œë ¥\"\"\"\n",
    "    param_counts = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        # ìµœìƒìœ„ ëª¨ë“ˆ ì´ë¦„ ì¶”ì¶œ\n",
    "        top_module = name.split('.')[0] + '.' + name.split('.')[1] if '.' in name else name\n",
    "        if top_module not in param_counts:\n",
    "            param_counts[top_module] = 0\n",
    "        param_counts[top_module] += param.numel()\n",
    "\n",
    "    total = sum(param_counts.values())\n",
    "    print(f\"\\nğŸ“Š {model_name} íŒŒë¼ë¯¸í„° ë¶„ì„:\")\n",
    "    print(f\"{'Module':<40} {'Parameters':>15} {'Percentage':>10}\")\n",
    "    print(\"-\" * 65)\n",
    "    for module, count in sorted(param_counts.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{module:<40} {count:>15,} {count/total*100:>9.2f}%\")\n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'TOTAL':<40} {total:>15,}\")\n",
    "    return total\n",
    "\n",
    "gpt1_total = count_parameters_by_module(gpt1_model, \"GPT-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì—°ìŠµ 2.1: GPT-1 ë‹¨ì¼ Transformer Block ë¶„ì„\n",
    "\n",
    "GPT-1ì˜ ë‹¨ì¼ Transformer blockì˜ íŒŒë¼ë¯¸í„° ìˆ˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ê³„ì‚°í•´ ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ì—°ìŠµ 2.1: GPT-1ì˜ ë‹¨ì¼ Transformer Block íŒŒë¼ë¯¸í„° ìˆ˜ ê³„ì‚°\n",
    "# ============================================================\n",
    "\n",
    "d_model = 768   # embedding dimension\n",
    "n_heads = 12    # number of attention heads\n",
    "d_ff = 3072     # feed-forward hidden dimension\n",
    "\n",
    "# TODO: ì•„ë˜ ë¹ˆì¹¸ì„ ì±„ìš°ì„¸ìš”\n",
    "# Hint: Multi-Head Attentionì—ëŠ” Q, K, V projection + Output projectionì´ ìˆìŠµë‹ˆë‹¤\n",
    "#       ê° projectionì€ (d_model x d_model) weight + d_model biasë¥¼ ê°€ì§‘ë‹ˆë‹¤\n",
    "\n",
    "# Multi-Head Attention íŒŒë¼ë¯¸í„°\n",
    "# Q, K, V ê°ê°: weight(d_model x d_model) + bias(d_model)\n",
    "attn_qkv_params = ___  # TODO: Q + K + V projection íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "attn_out_params = ___  # TODO: Output projection íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "attn_total = attn_qkv_params + attn_out_params\n",
    "\n",
    "# Feed-Forward Network íŒŒë¼ë¯¸í„°\n",
    "# ì²« ë²ˆì§¸ linear: d_model â†’ d_ff (weight + bias)\n",
    "# ë‘ ë²ˆì§¸ linear: d_ff â†’ d_model (weight + bias)\n",
    "ffn_params = ___  # TODO: FFN ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "\n",
    "# LayerNorm íŒŒë¼ë¯¸í„° (2ê°œ: attention í›„ + FFN í›„)\n",
    "# ê° LayerNorm: gamma(d_model) + beta(d_model)\n",
    "ln_params = ___  # TODO: LayerNorm íŒŒë¼ë¯¸í„° ìˆ˜\n",
    "\n",
    "block_total = attn_total + ffn_params + ln_params\n",
    "\n",
    "print(f\"Attention íŒŒë¼ë¯¸í„°: {attn_total:,}\")\n",
    "print(f\"FFN íŒŒë¼ë¯¸í„°:       {ffn_params:,}\")\n",
    "print(f\"LayerNorm íŒŒë¼ë¯¸í„°: {ln_params:,}\")\n",
    "print(f\"Block ì „ì²´:         {block_total:,}\")\n",
    "print(f\"12 Blocks ì „ì²´:     {block_total * 12:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ì •ë‹µ í™•ì¸: ì‹¤ì œ ëª¨ë¸ì—ì„œ ì¶”ì¶œí•˜ì—¬ ë¹„êµ\n",
    "# ============================================================\n",
    "\n",
    "block0 = gpt1_model.transformer.h[0]\n",
    "block_params_actual = sum(p.numel() for p in block0.parameters())\n",
    "print(f\"\\nâœ… ì‹¤ì œ GPT-1 Block 0 íŒŒë¼ë¯¸í„° ìˆ˜: {block_params_actual:,}\")\n",
    "\n",
    "# ê° ì„œë¸Œëª¨ë“ˆ í™•ì¸\n",
    "for name, module in block0.named_children():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"  {name}: {params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: GPT-1 â†’ GPT-2 í•µì‹¬ ë³€ê²½ì‚¬í•­\n",
    "\n",
    "GPT-2 (Radford et al., 2019)ëŠ” ë‹¨ìˆœíˆ ëª¨ë¸ì„ í‚¤ìš´ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ **ì•„í‚¤í…ì²˜ ìˆ˜ì¤€ì˜ ë³€ê²½**ì´ ìˆì—ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 1: Pre-Norm (LayerNorm ìœ„ì¹˜ ë³€ê²½)\n",
    "\n",
    "> \"*Layer normalization was moved to the input of each sub-block, similar to a pre-activation residual network,\n",
    "> and an additional layer normalization was added after the final self-attention block.*\"  \n",
    "> â€” Radford et al. (2019), \"Language Models are Unsupervised Multitask Learners\"\n",
    "\n",
    "**Post-Norm (GPT-1):**\n",
    "$$\\text{Output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))$$\n",
    "\n",
    "**Pre-Norm (GPT-2):**\n",
    "$$\\text{Output} = x + \\text{SubLayer}(\\text{LayerNorm}(x))$$\n",
    "\n",
    "Pre-Normì€ gradient flowë¥¼ ì•ˆì •í™”í•˜ì—¬ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "ì´ ë³€ê²½ ë•ë¶„ì— GPT-2ëŠ” ìµœëŒ€ 48 layers, GPT-3ëŠ” 96 layersê¹Œì§€ í™•ì¥ë  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 2: ì”ì°¨ ê²½ë¡œì˜ Weight Initialization Scaling\n",
    "\n",
    "> \"*We scale the weights of residual layers at initialization by a factor of $1/\\sqrt{N}$\n",
    "> where N is the number of residual layers.*\"  \n",
    "> â€” Radford et al. (2019)\n",
    "\n",
    "Residual connectionì´ ëˆ„ì ë˜ë©´ì„œ ë°œìƒí•˜ëŠ” ë¶„ì‚°(variance) ì¦ê°€ë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.\n",
    "ê° Transformer blockì—ëŠ” 2ê°œì˜ residual connectionì´ ìˆìœ¼ë¯€ë¡œ, $N = 2 \\times n_{\\text{layers}}$ì…ë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 3: Vocabulary & Context í™•ì¥\n",
    "\n",
    "- Vocabulary: 40,478 â†’ **50,257** (Byte-level BPE)\n",
    "- Context window: 512 â†’ **1,024** í† í°\n",
    "- Batch size: 64 â†’ **512**\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 4: íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ â€” Zero-shot Task Transfer\n",
    "\n",
    "GPT-1: Pre-train â†’ **Fine-tune** (ê° taskë³„ ì¶”ê°€ í•™ìŠµ í•„ìš”)  \n",
    "GPT-2: Pre-train â†’ **Zero-shot** (ì¶”ê°€ í•™ìŠµ ì—†ì´ ë°”ë¡œ task ìˆ˜í–‰)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ì‹¤ìŠµ: Pre-Norm vs Post-Norm ì‹œê°í™” ë° êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pre-Norm vs Post-Norm Transformer Block êµ¬í˜„\n",
    "# ============================================================\n",
    "\n",
    "class PostNormBlock(nn.Module):\n",
    "    \"\"\"GPT-1 ìŠ¤íƒ€ì¼: Post-Norm Transformer Block\n",
    "    Output = LayerNorm(x + SubLayer(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # Attention + Residual â†’ LayerNorm\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = self.ln1(x + self.dropout(attn_out))  # Post-Norm: LN after residual\n",
    "\n",
    "        # FFN + Residual â†’ LayerNorm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + ffn_out)  # Post-Norm: LN after residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class PreNormBlock(nn.Module):\n",
    "    \"\"\"GPT-2 ìŠ¤íƒ€ì¼: Pre-Norm Transformer Block\n",
    "    Output = x + SubLayer(LayerNorm(x))\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # LayerNorm â†’ Attention â†’ Residual\n",
    "        normed = self.ln1(x)  # Pre-Norm: LN before sublayer\n",
    "        attn_out, _ = self.attn(normed, normed, normed, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "\n",
    "        # LayerNorm â†’ FFN â†’ Residual\n",
    "        normed = self.ln2(x)  # Pre-Norm: LN before sublayer\n",
    "        ffn_out = self.ffn(normed)\n",
    "        x = x + ffn_out\n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"Post-Norm Block (GPT-1 style):\")\n",
    "print(PostNormBlock(768, 12, 3072))\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nPre-Norm Block (GPT-2 style):\")\n",
    "print(PreNormBlock(768, 12, 3072))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Pre-Norm vs Post-Normì—ì„œì˜ Gradient Flow ë¹„êµ ì‹¤í—˜\n",
    "# ============================================================\n",
    "\n",
    "def build_stacked_model(block_class, n_layers, d_model, n_heads, d_ff):\n",
    "    \"\"\"ì—¬ëŸ¬ ì¸µì˜ Transformer Blockì„ ìŒ“ì€ ëª¨ë¸ ìƒì„±\"\"\"\n",
    "    layers = nn.ModuleList([block_class(d_model, n_heads, d_ff) for _ in range(n_layers)])\n",
    "    return layers\n",
    "\n",
    "def measure_gradient_norms(block_class, n_layers_list, d_model=256, n_heads=4, d_ff=512):\n",
    "    \"\"\"ê° ì¸µ ìˆ˜ì— ëŒ€í•´ gradient normì„ ì¸¡ì •\"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for n_layers in n_layers_list:\n",
    "        torch.manual_seed(42)\n",
    "        layers = build_stacked_model(block_class, n_layers, d_model, n_heads, d_ff)\n",
    "\n",
    "        # Forward pass\n",
    "        x = torch.randn(2, 16, d_model, requires_grad=True)\n",
    "        h = x\n",
    "        for layer in layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        # Backward pass\n",
    "        loss = h.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        # ê° ì¸µì˜ attention weight gradient norm ê¸°ë¡\n",
    "        grad_norms = []\n",
    "        for i, layer in enumerate(layers):\n",
    "            for name, param in layer.named_parameters():\n",
    "                if 'in_proj_weight' in name and param.grad is not None:\n",
    "                    grad_norms.append(param.grad.norm().item())\n",
    "                    break\n",
    "\n",
    "        results[n_layers] = grad_norms\n",
    "\n",
    "    return results\n",
    "\n",
    "# ì‹¤í—˜ ì‹¤í–‰\n",
    "layer_counts = [4, 8, 16, 24]\n",
    "\n",
    "print(\"Gradient norm ì¸¡ì • ì¤‘...\")\n",
    "post_norm_grads = measure_gradient_norms(PostNormBlock, layer_counts)\n",
    "pre_norm_grads = measure_gradient_norms(PreNormBlock, layer_counts)\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, len(layer_counts), figsize=(20, 5))\n",
    "fig.suptitle('Gradient Norms: Post-Norm (GPT-1) vs Pre-Norm (GPT-2)', fontsize=16, fontweight='bold')\n",
    "\n",
    "for idx, n_layers in enumerate(layer_counts):\n",
    "    ax = axes[idx]\n",
    "    layers_range = range(len(post_norm_grads[n_layers]))\n",
    "\n",
    "    ax.plot(layers_range, post_norm_grads[n_layers], 'r-o', label='Post-Norm (GPT-1)', alpha=0.7)\n",
    "    ax.plot(layers_range, pre_norm_grads[n_layers], 'b-s', label='Pre-Norm (GPT-2)', alpha=0.7)\n",
    "    ax.set_title(f'{n_layers} Layers')\n",
    "    ax.set_xlabel('Layer Index')\n",
    "    ax.set_ylabel('Gradient Norm')\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ ê´€ì°°: Pre-Normì€ ê¹Šì€ ì¸µì—ì„œë„ gradientê°€ ë” ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€ë©ë‹ˆë‹¤.\")\n",
    "print(\"   ì´ê²ƒì´ GPT-2/3ê°€ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì—ˆë˜ í•µì‹¬ ì´ìœ  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ì‹¤ìŠµ: Weight Initialization Scaling ($1/\\sqrt{N}$) ì‹¤í—˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Residual streamì˜ ë¶„ì‚° í­ë°œ ë¬¸ì œì™€ 1/âˆšN scalingì˜ íš¨ê³¼\n",
    "# ============================================================\n",
    "\n",
    "def simulate_residual_variance(n_layers, use_scaling=False):\n",
    "    \"\"\"\n",
    "    Residual connectionì„ í†µê³¼í•˜ë©´ì„œ activationsì˜ ë¶„ì‚°ì´\n",
    "    ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‹œë®¬ë ˆì´ì…˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    use_scaling=Trueì¼ ë•Œ GPT-2ì˜ 1/âˆšN scalingì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    d_model = 256\n",
    "    batch_size = 32\n",
    "    seq_len = 64\n",
    "    N = 2 * n_layers  # ê° blockì— 2ê°œì˜ residual connection\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    variances = [x.var().item()]\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        # Attention sublayerì˜ output ì‹œë®¬ë ˆì´ì…˜\n",
    "        attn_out = torch.randn_like(x) * 0.02  # ì´ˆê¸° ê°€ì¤‘ì¹˜ë¡œ ì¸í•œ ì‘ì€ ì¶œë ¥\n",
    "\n",
    "        if use_scaling:\n",
    "            # GPT-2 scaling: ì”ì°¨ ì¸µì˜ ì¶œë ¥ì„ 1/âˆšNìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§\n",
    "            attn_out = attn_out / np.sqrt(N)\n",
    "\n",
    "        x = x + attn_out  # Residual connection\n",
    "        variances.append(x.var().item())\n",
    "\n",
    "        # FFN sublayerì˜ output ì‹œë®¬ë ˆì´ì…˜\n",
    "        ffn_out = torch.randn_like(x) * 0.02\n",
    "\n",
    "        if use_scaling:\n",
    "            ffn_out = ffn_out / np.sqrt(N)\n",
    "\n",
    "        x = x + ffn_out  # Residual connection\n",
    "        variances.append(x.var().item())\n",
    "\n",
    "    return variances\n",
    "\n",
    "# 48 layers (GPT-2 XLê³¼ ë™ì¼)ì—ì„œ ë¹„êµ\n",
    "n_layers = 48\n",
    "var_no_scale = simulate_residual_variance(n_layers, use_scaling=False)\n",
    "var_with_scale = simulate_residual_variance(n_layers, use_scaling=True)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(var_no_scale, 'r-', linewidth=2, label='Without scaling')\n",
    "ax1.plot(var_with_scale, 'b-', linewidth=2, label='With 1/âˆšN scaling (GPT-2)')\n",
    "ax1.set_xlabel('Residual Connection Index')\n",
    "ax1.set_ylabel('Activation Variance')\n",
    "ax1.set_title(f'Activation Variance over {n_layers} layers')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# ì •ê·œí™”í•´ì„œ ë¹„ìœ¨ ë¹„êµ\n",
    "ax2.plot(np.array(var_no_scale) / var_no_scale[0], 'r-', linewidth=2, label='Without scaling')\n",
    "ax2.plot(np.array(var_with_scale) / var_with_scale[0], 'b-', linewidth=2, label='With 1/âˆšN scaling (GPT-2)')\n",
    "ax2.axhline(y=1.0, color='gray', linestyle='--', alpha=0.5, label='Initial variance (baseline)')\n",
    "ax2.set_xlabel('Residual Connection Index')\n",
    "ax2.set_ylabel('Variance Ratio (relative to initial)')\n",
    "ax2.set_title('Normalized Variance Growth')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Scaling ì—†ì´ {n_layers}ì¸µ í›„ ë¶„ì‚° ë³€í™”: {var_no_scale[0]:.4f} â†’ {var_no_scale[-1]:.4f} ({var_no_scale[-1]/var_no_scale[0]:.2f}x)\")\n",
    "print(f\"ğŸ“‰ 1/âˆšN Scaling ì ìš© í›„ ë¶„ì‚° ë³€í™”: {var_with_scale[0]:.4f} â†’ {var_with_scale[-1]:.4f} ({var_with_scale[-1]/var_with_scale[0]:.2f}x)\")\n",
    "print(f\"\\nğŸ’¡ 1/âˆšN scalingì´ residual streamì˜ ë¶„ì‚°ì„ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€ì‹œí‚¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ì‹¤ìŠµ: ì‹¤ì œ GPT-1 vs GPT-2 ëª¨ë¸ êµ¬ì¡° ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GPT-2 ëª¨ë¸ ë¡œë“œ ë° GPT-1ê³¼ êµ¬ì¡° ë¹„êµ\n",
    "# ============================================================\n",
    "\n",
    "gpt2_config = GPT2Config()  # GPT-2 small (124M) ê¸°ë³¸ ì„¤ì •\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "print(\"ğŸ“‹ GPT-1 vs GPT-2 Configuration ë¹„êµ:\")\n",
    "print(f\"{'í•­ëª©':<30} {'GPT-1':>15} {'GPT-2 (small)':>15}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Vocab size':<30} {gpt1_config.vocab_size:>15,} {gpt2_config.vocab_size:>15,}\")\n",
    "print(f\"{'Embedding dim':<30} {gpt1_config.n_embd:>15} {gpt2_config.n_embd:>15}\")\n",
    "print(f\"{'Num layers':<30} {gpt1_config.n_layer:>15} {gpt2_config.n_layer:>15}\")\n",
    "print(f\"{'Num heads':<30} {gpt1_config.n_head:>15} {gpt2_config.n_head:>15}\")\n",
    "print(f\"{'Max positions (context)':<30} {gpt1_config.n_positions:>15} {gpt2_config.n_positions:>15}\")\n",
    "\n",
    "gpt1_params = sum(p.numel() for p in gpt1_model.parameters())\n",
    "gpt2_params = sum(p.numel() for p in gpt2_model.parameters())\n",
    "print(f\"{'Total parameters':<30} {gpt1_params:>15,} {gpt2_params:>15,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# í•µì‹¬ êµ¬ì¡° ì°¨ì´ í™•ì¸: LayerNorm ìœ„ì¹˜\n",
    "# ============================================================\n",
    "\n",
    "print(\"ğŸ” GPT-1 Block 0 êµ¬ì¡° (Post-Norm):\")\n",
    "print(\"-\" * 40)\n",
    "for name, _ in gpt1_model.transformer.h[0].named_children():\n",
    "    print(f\"  {name}\")\n",
    "\n",
    "print(f\"\\nğŸ” GPT-2 Block 0 êµ¬ì¡° (Pre-Norm):\")\n",
    "print(\"-\" * 40)\n",
    "for name, _ in gpt2_model.transformer.h[0].named_children():\n",
    "    print(f\"  {name}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ì£¼ëª©í•  ì :\")\n",
    "print(\"  - GPT-2ì—ëŠ” 'ln_1', 'ln_2'ê°€ ê° sublayer ì „ì— ìœ„ì¹˜ (Pre-Norm)\")\n",
    "print(\"  - GPT-2ì—ëŠ” ìµœì¢… ì¶œë ¥ ì „ì— ì¶”ê°€ LayerNorm ('ln_f')ì´ ìˆìŒ:\")\n",
    "print(f\"  - GPT-2 final LayerNorm: {gpt2_model.transformer.ln_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì—°ìŠµ 3.1: GPT-2ì˜ Weight Initialization Scaling í™•ì¸\n",
    "\n",
    "GPT-2 ë…¼ë¬¸ì— ë”°ë¥´ë©´, residual layerì˜ projection weightëŠ” $1/\\sqrt{N}$ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ë©ë‹ˆë‹¤.\n",
    "ì‹¤ì œ HuggingFace ì½”ë“œì—ì„œ ì´ë¥¼ í™•ì¸í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ì—°ìŠµ 3.1: GPT-2 ì´ˆê¸°í™” ì‹œ projection weightì˜ std ë¶„ì„\n",
    "# ============================================================\n",
    "\n",
    "# ìƒˆë¡œìš´ (ì‚¬ì „í•™ìŠµë˜ì§€ ì•Šì€) GPT-2 ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤\n",
    "fresh_gpt2 = GPT2LMHeadModel(GPT2Config())\n",
    "\n",
    "# Attentionì˜ c_proj (output projection)ì™€ FFNì˜ c_proj weightì˜ stdë¥¼ í™•ì¸í•©ë‹ˆë‹¤\n",
    "attn_proj_stds = []\n",
    "ffn_proj_stds = []\n",
    "other_stds = []\n",
    "\n",
    "for name, param in fresh_gpt2.named_parameters():\n",
    "    if 'c_proj.weight' in name and 'attn' in name:\n",
    "        attn_proj_stds.append((name, param.std().item()))\n",
    "    elif 'c_proj.weight' in name and 'mlp' in name:\n",
    "        ffn_proj_stds.append((name, param.std().item()))\n",
    "    elif 'c_attn.weight' in name:\n",
    "        other_stds.append((name, param.std().item()))\n",
    "\n",
    "print(\"ğŸ“Š GPT-2 Weight Initialization Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ì´ë¡ ê°’ ê³„ì‚°\n",
    "n_layer = GPT2Config().n_layer  # 12\n",
    "base_std = 0.02\n",
    "N_residual = 2 * n_layer  # 24\n",
    "expected_proj_std = base_std / np.sqrt(N_residual)\n",
    "\n",
    "print(f\"\\nì´ë¡ ì  ê¸°ëŒ€ê°’:\")\n",
    "print(f\"  - ì¼ë°˜ weight std: {base_std}\")\n",
    "print(f\"  - Projection weight std: {base_std} / âˆš{N_residual} = {expected_proj_std:.6f}\")\n",
    "\n",
    "print(f\"\\nì‹¤ì œ Attention c_proj weight std (ì²˜ìŒ 3ê°œ):\")\n",
    "for name, std in attn_proj_stds[:3]:\n",
    "    print(f\"  {name}: {std:.6f}\")\n",
    "\n",
    "print(f\"\\nì‹¤ì œ FFN c_proj weight std (ì²˜ìŒ 3ê°œ):\")\n",
    "for name, std in ffn_proj_stds[:3]:\n",
    "    print(f\"  {name}: {std:.6f}\")\n",
    "\n",
    "print(f\"\\nì¼ë°˜ c_attn weight std (ì²˜ìŒ 3ê°œ):\")\n",
    "for name, std in other_stds[:3]:\n",
    "    print(f\"  {name}: {std:.6f}\")\n",
    "\n",
    "# TODO: ì•„ë˜ ì§ˆë¬¸ì— ë‹µí•´ë³´ì„¸ìš”\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"â“ ì§ˆë¬¸: c_proj weightì˜ stdê°€ ì¼ë°˜ weightì˜ stdë³´ë‹¤ ì‘ì€ ì´ìœ ëŠ” ë¬´ì—‡ì¼ê¹Œìš”?\")\n",
    "print(\"   Hint: Residual connectionì˜ ëˆ„ì  íš¨ê³¼ë¥¼ ìƒê°í•´ë³´ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: GPT-2 â†’ GPT-3 í•µì‹¬ ë³€ê²½ì‚¬í•­\n",
    "\n",
    "GPT-3 (Brown et al., 2020)ëŠ” GPT-2ì˜ ì•„í‚¤í…ì²˜ë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ì„œë„ ë‘ ê°€ì§€ í•µì‹¬ í˜ì‹ ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 1: Alternating Dense + Sparse Attention\n",
    "\n",
    "> \"*We use the same model and architecture as GPT-2, including the modified initialization,\n",
    "> pre-normalization, and reversible tokenization described therein, with the exception that\n",
    "> we use alternating dense and locally banded sparse attention patterns in the layers\n",
    "> of the transformer, similar to the Sparse Transformer.*\"  \n",
    "> â€” Brown et al. (2020), \"Language Models are Few-Shot Learners\"\n",
    "\n",
    "- **Dense (Full) Attention**: ëª¨ë“  ì´ì „ í† í°ì— attend (ì¼ë°˜ì ì¸ ë°©ì‹)\n",
    "- **Locally Banded Sparse Attention**: ì¸ì ‘í•œ ì¼ì • ë²”ìœ„ì˜ í† í°ì—ë§Œ attend\n",
    "- ì´ ë‘ íŒ¨í„´ì„ **ë²ˆê°ˆì•„ê°€ë©°** ì‚¬ìš© (Layer 0: Dense, Layer 1: Sparse, Layer 2: Dense, ...)\n",
    "\n",
    "ì´ë¥¼ í†µí•´ 96 layersì˜ ê¹Šì€ ëª¨ë¸ì—ì„œë„ computational costë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 2: In-Context Learning íŒ¨ëŸ¬ë‹¤ì„\n",
    "\n",
    "GPT-1: Pre-train â†’ **Fine-tune**  \n",
    "GPT-2: Pre-train â†’ **Zero-shot** (task descriptionë§Œìœ¼ë¡œ ìˆ˜í–‰)  \n",
    "GPT-3: Pre-train â†’ **Few-shot / In-context Learning** (ëª‡ ê°œì˜ ì˜ˆì‹œë¥¼ promptì— í¬í•¨)\n",
    "\n",
    "> GPT-3 ë…¼ë¬¸ì—ì„œëŠ” **Zero-shot**, **One-shot**, **Few-shot** ì„¸ ê°€ì§€ in-context learning ì„¤ì •ì„\n",
    "> ì²´ê³„ì ìœ¼ë¡œ ì •ì˜í•˜ê³ , fine-tuning ì—†ì´ë„ taskë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### ğŸ”‘ ë³€ê²½ì‚¬í•­ 3: ê·¹ë‹¨ì  ìŠ¤ì¼€ì¼ë§\n",
    "\n",
    "- 175B parameters (GPT-2 ëŒ€ë¹„ ~117x)\n",
    "- 96 attention layers\n",
    "- d_model = 12,288 / 96 heads / d_head = 128\n",
    "- í•™ìŠµ ë°ì´í„°: ~300B í† í° (5ê°œ ë°ì´í„°ì†ŒìŠ¤ì˜ ê°€ì¤‘ í•©)\n",
    "- Context window: 2,048 í† í°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 ì‹¤ìŠµ: Dense vs Sparse Attention ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Dense vs Locally-Banded Sparse Attention íŒ¨í„´ ì‹œê°í™”\n",
    "# ============================================================\n",
    "\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Standard causal (dense) attention mask\"\"\"\n",
    "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    return mask\n",
    "\n",
    "def create_sparse_banded_mask(seq_len, bandwidth=4):\n",
    "    \"\"\"Locally-banded sparse attention mask (GPT-3 style)\"\"\"\n",
    "    causal = torch.tril(torch.ones(seq_len, seq_len))\n",
    "    band = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - bandwidth + 1)\n",
    "        band[i, start:i+1] = 1.0\n",
    "    return causal * band\n",
    "\n",
    "seq_len = 16\n",
    "bandwidth = 4  # ì‹¤ì œ GPT-3ì—ì„œëŠ” ë” í° window ì‚¬ìš©\n",
    "\n",
    "dense_mask = create_causal_mask(seq_len)\n",
    "sparse_mask = create_sparse_banded_mask(seq_len, bandwidth)\n",
    "\n",
    "# GPT-3 ìŠ¤íƒ€ì¼: alternating pattern\n",
    "fig, axes = plt.subplots(1, 4, figsize=(22, 5))\n",
    "\n",
    "titles = [\n",
    "    'Layer 0: Dense\\n(Full Causal)',\n",
    "    'Layer 1: Sparse\\n(Locally Banded)',\n",
    "    'Layer 2: Dense\\n(Full Causal)',\n",
    "    'Layer 3: Sparse\\n(Locally Banded)'\n",
    "]\n",
    "\n",
    "masks = [dense_mask, sparse_mask, dense_mask, sparse_mask]\n",
    "colors = ['Blues', 'Oranges', 'Blues', 'Oranges']\n",
    "\n",
    "for idx, (ax, title, mask, cmap) in enumerate(zip(axes, titles, masks, colors)):\n",
    "    sns.heatmap(mask.numpy(), ax=ax, cmap=cmap, cbar=False,\n",
    "                square=True, linewidths=0.5, linecolor='white')\n",
    "    ax.set_title(title, fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel('Key Position')\n",
    "    ax.set_ylabel('Query Position')\n",
    "\n",
    "plt.suptitle('GPT-3: Alternating Dense & Sparse Attention Patterns',\n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ê³„ì‚°ëŸ‰ ë¹„êµ\n",
    "dense_ops = dense_mask.sum().item()\n",
    "sparse_ops = sparse_mask.sum().item()\n",
    "print(f\"\\nğŸ“Š Attention ì—°ì‚°ëŸ‰ ë¹„êµ (sequence length = {seq_len}):\")\n",
    "print(f\"  Dense layer: {int(dense_ops)} attention connections\")\n",
    "print(f\"  Sparse layer: {int(sparse_ops)} attention connections\")\n",
    "print(f\"  Sparse layer ì ˆì•½: {(1 - sparse_ops/dense_ops)*100:.1f}%\")\n",
    "print(f\"  Alternating í‰ê·  ì ˆì•½: {(1 - (dense_ops+sparse_ops)/(2*dense_ops))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì—°ìŠµ 4.1: Sparse Attentionì˜ ë³µì¡ë„ ë¶„ì„\n",
    "\n",
    "Dense attentionì€ $O(n^2)$, Locally-banded sparse attentionì€ $O(n \\cdot w)$ ($w$=window size)ì˜\n",
    "ë³µì¡ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì•„ë˜ ì½”ë“œë¥¼ ì™„ì„±í•˜ì—¬ ì´ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ì—°ìŠµ 4.1: Attention ë³µì¡ë„ ë¶„ì„\n",
    "# ============================================================\n",
    "\n",
    "seq_lengths = [64, 128, 256, 512, 1024, 2048, 4096]\n",
    "window_size = 256  # ëŒ€í‘œì ì¸ sparse attention window size\n",
    "\n",
    "# TODO: ì•„ë˜ ë¹ˆì¹¸ì„ ì±„ìš°ì„¸ìš”\n",
    "dense_complexity = [___  for n in seq_lengths]     # O(n^2): ê° í† í°ì´ ëª¨ë“  ì´ì „ í† í° ì°¸ì¡°\n",
    "sparse_complexity = [___ for n in seq_lengths]     # O(n*w): ê° í† í°ì´ window ë‚´ í† í°ë§Œ ì°¸ì¡°\n",
    "alternating_complexity = [___ for n in seq_lengths] # í‰ê· : (dense + sparse) / 2\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(seq_lengths, dense_complexity, 'r-o', linewidth=2, label='Dense Attention: O(nÂ²)')\n",
    "plt.plot(seq_lengths, sparse_complexity, 'g-s', linewidth=2, label=f'Sparse Attention: O(nÂ·{window_size})')\n",
    "plt.plot(seq_lengths, alternating_complexity, 'b-^', linewidth=2, label='GPT-3 Alternating (average)')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Number of Attention Operations')\n",
    "plt.title('Attention Complexity: Dense vs Sparse vs GPT-3 Alternating')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# n=2048 (GPT-3ì˜ context length)ì—ì„œ ë¹„êµ\n",
    "n = 2048\n",
    "print(f\"\\nğŸ“Š n = {n} (GPT-3 context length)ì—ì„œì˜ ë¹„êµ:\")\n",
    "print(f\"  Dense: {n*n:,} operations\")\n",
    "print(f\"  Sparse (w={window_size}): {n*window_size:,} operations\")\n",
    "print(f\"  GPT-3 Alternating: {(n*n + n*window_size)//2:,} operations\")\n",
    "print(f\"  ì ˆì•½ë¥ : {(1 - (n*n + n*window_size)/(2*n*n))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 GPT-3ì˜ í•™ìŠµ ë°ì´í„° êµ¬ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GPT-3 Training Data Composition ì‹œê°í™”\n",
    "# ============================================================\n",
    "# Source: Brown et al. (2020), Table 2.2\n",
    "\n",
    "data_sources = {\n",
    "    'Dataset': ['Common Crawl\\n(filtered)', 'WebText2', 'Books1', 'Books2', 'Wikipedia'],\n",
    "    'Tokens (Billions)': [410, 19, 12, 55, 3],\n",
    "    'Weight in Training Mix (%)': [60, 22, 8, 8, 3],\n",
    "    'Epochs': [0.44, 2.9, 1.9, 0.43, 3.4]\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# 1. Raw token count\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']\n",
    "axes[0].barh(data_sources['Dataset'], data_sources['Tokens (Billions)'], color=colors, edgecolor='white')\n",
    "axes[0].set_xlabel('Tokens (Billions)')\n",
    "axes[0].set_title('Raw Token Count by Source', fontweight='bold')\n",
    "for i, v in enumerate(data_sources['Tokens (Billions)']):\n",
    "    axes[0].text(v + 5, i, f'{v}B', va='center', fontweight='bold')\n",
    "\n",
    "# 2. Training weight\n",
    "axes[1].pie(data_sources['Weight in Training Mix (%)'],\n",
    "           labels=data_sources['Dataset'],\n",
    "           autopct='%1.0f%%',\n",
    "           colors=colors,\n",
    "           startangle=90,\n",
    "           textprops={'fontsize': 10})\n",
    "axes[1].set_title('Weight in Training Mix', fontweight='bold')\n",
    "\n",
    "# 3. Epochs per dataset\n",
    "bars = axes[2].barh(data_sources['Dataset'], data_sources['Epochs'], color=colors, edgecolor='white')\n",
    "axes[2].axvline(x=1.0, color='red', linestyle='--', alpha=0.5, label='1 epoch')\n",
    "axes[2].set_xlabel('Epochs')\n",
    "axes[2].set_title('Epochs per Dataset', fontweight='bold')\n",
    "axes[2].legend()\n",
    "for i, v in enumerate(data_sources['Epochs']):\n",
    "    axes[2].text(v + 0.05, i, f'{v}x', va='center', fontweight='bold')\n",
    "\n",
    "plt.suptitle('GPT-3 Training Data Composition\\n(Brown et al., 2020)',\n",
    "             fontsize=16, fontweight='bold', y=1.03)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¡ í•µì‹¬ ê´€ì°°:\")\n",
    "print(\"  1. Common Crawlì´ ì „ì²´ í† í°ì˜ 82%ë¥¼ ì°¨ì§€í•˜ì§€ë§Œ, í•™ìŠµ ì‹œì—ëŠ” 60%ë¡œ ë‹¤ìš´ìƒ˜í”Œë§\")\n",
    "print(\"  2. ê³ í’ˆì§ˆ ë°ì´í„°(WebText2, Wikipedia)ëŠ” 2-3 ì—í­ ë°˜ë³µ í•™ìŠµ (ì˜¤ë²„í”¼íŒ… í—ˆìš©)\")\n",
    "print(\"  3. ì €í’ˆì§ˆ ëŒ€ê·œëª¨ ë°ì´í„°(Common Crawl)ëŠ” 1 ì—í­ ë¯¸ë§Œìœ¼ë¡œ í•™ìŠµ\")\n",
    "print(\"  4. ì´ í•™ìŠµ í† í°: ~300B (ì „ì²´ 499B ì¤‘ ê°€ì¤‘ ìƒ˜í”Œë§)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: In-Context Learning ì‹¤í—˜ (GPT-2ë¡œ ì²´í—˜)\n",
    "\n",
    "GPT-3ì˜ í•µì‹¬ í˜ì‹ ì¸ **in-context learning**ì„ GPT-2 ëª¨ë¸ë¡œ ê°„ì ‘ ì²´í—˜í•´ë´…ë‹ˆë‹¤.\n",
    "\n",
    "- **Zero-shot**: Task descriptionë§Œ ì œê³µ\n",
    "- **One-shot**: Task description + 1ê°œ ì˜ˆì‹œ\n",
    "- **Few-shot**: Task description + ì—¬ëŸ¬ ì˜ˆì‹œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# In-Context Learning ì‹¤í—˜\n",
    "# GPT-2ë¥¼ ì‚¬ìš©í•˜ì—¬ Zero-shot / One-shot / Few-shot ë¹„êµ\n",
    "# ============================================================\n",
    "\n",
    "# GPT-2 medium ë¡œë“œ (ë” í° ëª¨ë¸ì¼ìˆ˜ë¡ in-context learning ëŠ¥ë ¥ í–¥ìƒ)\n",
    "print(\"GPT-2 ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "generator = pipeline('text-generation', model='gpt2', device=-1)  # CPU\n",
    "print(\"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "# Sentiment Analysisë¥¼ í†µí•œ in-context learning ì‹¤í—˜\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ“ ì‹¤í—˜: Sentiment Classification via In-Context Learning\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Zero-shot\n",
    "zero_shot_prompt = \"\"\"Review: This movie was absolutely wonderful and I loved every moment of it.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# One-shot\n",
    "one_shot_prompt = \"\"\"Review: I hated this movie, it was terrible.\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: This movie was absolutely wonderful and I loved every moment of it.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# Few-shot\n",
    "few_shot_prompt = \"\"\"Review: I hated this movie, it was terrible.\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: Best film I have seen this year!\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: Not worth watching, very boring.\n",
    "Sentiment: Negative\n",
    "\n",
    "Review: An outstanding masterpiece of cinema.\n",
    "Sentiment: Positive\n",
    "\n",
    "Review: This movie was absolutely wonderful and I loved every moment of it.\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "prompts = {\n",
    "    'Zero-shot': zero_shot_prompt,\n",
    "    'One-shot': one_shot_prompt,\n",
    "    'Few-shot (4 examples)': few_shot_prompt\n",
    "}\n",
    "\n",
    "for setting, prompt in prompts.items():\n",
    "    print(f\"\\n{'â”€' * 50}\")\n",
    "    print(f\"ğŸ”¬ {setting}\")\n",
    "    print(f\"{'â”€' * 50}\")\n",
    "\n",
    "    result = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=5,\n",
    "        num_return_sequences=3,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=generator.tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    for i, r in enumerate(result):\n",
    "        generated = r['generated_text'][len(prompt):].strip().split('\\n')[0]\n",
    "        print(f\"  Output {i+1}: {generated}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ’¡ ê´€ì°°:\")\n",
    "print(\"  - Few-shotì—ì„œ 'Positive'ë¥¼ ì¶œë ¥í•  í™•ë¥ ì´ ë†’ì•„ì§ì„ í™•ì¸í•˜ì„¸ìš”\")\n",
    "print(\"  - GPT-2ëŠ” GPT-3ë³´ë‹¤ ì‘ì•„ì„œ in-context learning ëŠ¥ë ¥ì´ ì œí•œì ì´ì§€ë§Œ,\")\n",
    "print(\"    íŒ¨í„´ì„ ë”°ë¼í•˜ë ¤ëŠ” ê²½í–¥ì„ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\")\n",
    "print(\"  - GPT-3 (175B)ì—ì„œëŠ” ì´ ëŠ¥ë ¥ì´ ê·¹ì ìœ¼ë¡œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ ì—°ìŠµ 5.1: ìì‹ ë§Œì˜ In-Context Learning ì‹¤í—˜\n",
    "\n",
    "ì•„ë˜ ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì—¬ **ë²ˆì—­(Translation)** ë˜ëŠ” **ìš”ì•½(Summarization)** ë“±\n",
    "ë‹¤ë¥¸ taskì— ëŒ€í•´ in-context learningì„ ì‹¤í—˜í•´ë³´ì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ì—°ìŠµ 5.1: ë‚˜ë§Œì˜ In-Context Learning ì‹¤í—˜\n",
    "# ============================================================\n",
    "\n",
    "# TODO: ì•„ë˜ promptë¥¼ ìˆ˜ì •í•˜ì—¬ ë‹¤ë¥¸ taskë¥¼ ì‹¤í—˜í•´ë³´ì„¸ìš”\n",
    "# ì˜ˆì‹œ: ë²ˆì—­, ë¶„ë¥˜, ë³€í™˜ ë“±\n",
    "\n",
    "my_prompt = \"\"\"\n",
    "English: Hello, how are you?\n",
    "Korean: ì•ˆë…•í•˜ì„¸ìš”, ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?\n",
    "\n",
    "English: Thank you very much.\n",
    "Korean: ì •ë§ ê°ì‚¬í•©ë‹ˆë‹¤.\n",
    "\n",
    "English: I love studying natural language processing.\n",
    "Korean:\"\"\"\n",
    "\n",
    "result = generator(\n",
    "    my_prompt,\n",
    "    max_new_tokens=20,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    "    pad_token_id=generator.tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "print(\"ğŸ”¬ ë‚˜ì˜ In-Context Learning ì‹¤í—˜ ê²°ê³¼:\")\n",
    "for i, r in enumerate(result):\n",
    "    generated = r['generated_text'][len(my_prompt):].strip().split('\\n')[0]\n",
    "    print(f\"  Output {i+1}: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: ì „ì²´ íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ ë¹„êµ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GPT-1/2/3 íŒŒë¼ë¯¸í„° ìŠ¤ì¼€ì¼ ë¹„êµ\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 7))\n",
    "\n",
    "# 1. íŒŒë¼ë¯¸í„° ìˆ˜ ë¹„êµ (ë¡œê·¸ ìŠ¤ì¼€ì¼)\n",
    "models = ['GPT-1\\n(2018)', 'GPT-2 Small\\n(2019)', 'GPT-2 Medium', 'GPT-2 Large', 'GPT-2 XL', 'GPT-3\\n(2020)']\n",
    "params = [0.117, 0.124, 0.355, 0.774, 1.5, 175]\n",
    "colors_bar = ['#3498db', '#2ecc71', '#2ecc71', '#2ecc71', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = axes[0].bar(models, params, color=colors_bar, edgecolor='white', linewidth=1.5)\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].set_ylabel('Parameters (Billions)', fontsize=12)\n",
    "axes[0].set_title('Model Size Evolution', fontsize=14, fontweight='bold')\n",
    "for bar, p in zip(bars, params):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., bar.get_height() * 1.2,\n",
    "                f'{p}B', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 2. ì•„í‚¤í…ì²˜ ì°¨ì› ë¹„êµ\n",
    "arch_data = {\n",
    "    'Model': ['GPT-1', 'GPT-2 XL', 'GPT-3'],\n",
    "    'Layers': [12, 48, 96],\n",
    "    'd_model': [768, 1600, 12288],\n",
    "    'Heads': [12, 25, 96],\n",
    "    'Context': [512, 1024, 2048]\n",
    "}\n",
    "\n",
    "x = np.arange(len(arch_data['Model']))\n",
    "width = 0.2\n",
    "\n",
    "# ì •ê·œí™”í•˜ì—¬ ë¹„êµ\n",
    "for i, (metric, values) in enumerate([(k, v) for k, v in arch_data.items() if k != 'Model']):\n",
    "    normalized = [v / max(values) for v in values]\n",
    "    bars = axes[1].bar(x + i*width, normalized, width, label=metric, alpha=0.8)\n",
    "    for bar, orig in zip(bars, values):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02,\n",
    "                    str(orig), ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "axes[1].set_xticks(x + width * 1.5)\n",
    "axes[1].set_xticklabels(arch_data['Model'])\n",
    "axes[1].set_ylabel('Normalized Value', fontsize=12)\n",
    "axes[1].set_title('Architecture Dimensions', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=9)\n",
    "\n",
    "# 3. í•™ìŠµ ë°ì´í„° í¬ê¸° ë¹„êµ\n",
    "data_models = ['GPT-1\\n(BooksCorpus)', 'GPT-2\\n(WebText)', 'GPT-3\\n(Mixed)']\n",
    "data_sizes = [5, 40, 570]  # GB\n",
    "data_colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = axes[2].bar(data_models, data_sizes, color=data_colors, edgecolor='white')\n",
    "axes[2].set_ylabel('Training Data Size (GB)', fontsize=12)\n",
    "axes[2].set_title('Training Data Scale', fontsize=14, fontweight='bold')\n",
    "for bar, s in zip(bars, data_sizes):\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 5,\n",
    "                f'{s} GB', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('GPT Series: Scale Comparison', fontsize=18, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 7: ì§„í™”ì˜ í•µì‹¬ì„ ìš”ì•½í•˜ëŠ” ë‹¤ì´ì–´ê·¸ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GPT ì‹œë¦¬ì¦ˆ ì§„í™” íƒ€ì„ë¼ì¸ ë‹¤ì´ì–´ê·¸ë¨\n",
    "# ============================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.set_xlim(0, 30)\n",
    "ax.set_ylim(0, 20)\n",
    "ax.axis('off')\n",
    "\n",
    "# íƒ€ì´í‹€\n",
    "ax.text(15, 19, 'Evolution of the GPT Architecture',\n",
    "        fontsize=22, fontweight='bold', ha='center', va='center',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor='#2C3E50', edgecolor='none'),\n",
    "        color='white')\n",
    "\n",
    "# GPT-1 Box\n",
    "gpt1_box = mpatches.FancyBboxPatch((1, 10), 7, 7,\n",
    "    boxstyle=mpatches.BoxStyle(\"Round\", pad=0.3),\n",
    "    facecolor='#3498db', edgecolor='white', linewidth=2, alpha=0.9)\n",
    "ax.add_patch(gpt1_box)\n",
    "ax.text(4.5, 16.2, 'GPT-1 (Jun 2018)', fontsize=14, fontweight='bold', ha='center', color='white')\n",
    "ax.text(4.5, 15.2, '117M params', fontsize=11, ha='center', color='white')\n",
    "gpt1_features = [\n",
    "    'â€¢ Post-Norm (original Transformer)',\n",
    "    'â€¢ 12 layers, d=768, 12 heads',\n",
    "    'â€¢ Context: 512 tokens',\n",
    "    'â€¢ BooksCorpus (~5GB)',\n",
    "    'â€¢ Pre-train â†’ Fine-tune',\n",
    "    'â€¢ BPE vocab: 40,478'\n",
    "]\n",
    "for i, feat in enumerate(gpt1_features):\n",
    "    ax.text(1.5, 14.2 - i * 0.7, feat, fontsize=9, color='white', family='monospace')\n",
    "\n",
    "# Arrow 1\n",
    "ax.annotate('', xy=(10, 13.5), xytext=(8.5, 13.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#E74C3C'))\n",
    "\n",
    "# GPT-2 Box\n",
    "gpt2_box = mpatches.FancyBboxPatch((10.5, 10), 8, 7,\n",
    "    boxstyle=mpatches.BoxStyle(\"Round\", pad=0.3),\n",
    "    facecolor='#27AE60', edgecolor='white', linewidth=2, alpha=0.9)\n",
    "ax.add_patch(gpt2_box)\n",
    "ax.text(14.5, 16.2, 'GPT-2 (Feb 2019)', fontsize=14, fontweight='bold', ha='center', color='white')\n",
    "ax.text(14.5, 15.2, '1.5B params (13x)', fontsize=11, ha='center', color='white')\n",
    "gpt2_features = [\n",
    "    'â˜… Pre-Norm LayerNorm',\n",
    "    'â˜… Weight init: 1/âˆšN scaling',\n",
    "    'â˜… Final LayerNorm added',\n",
    "    'â€¢ Context: 512 â†’ 1024 tokens',\n",
    "    'â€¢ WebText (~40GB)',\n",
    "    'â˜… Zero-shot task transfer',\n",
    "    'â€¢ Byte-level BPE: 50,257'\n",
    "]\n",
    "for i, feat in enumerate(gpt2_features):\n",
    "    color = '#FFEB3B' if feat.startswith('â˜…') else 'white'\n",
    "    ax.text(11, 14.2 - i * 0.65, feat, fontsize=9, color=color, family='monospace')\n",
    "\n",
    "# Arrow 2\n",
    "ax.annotate('', xy=(20, 13.5), xytext=(19, 13.5),\n",
    "            arrowprops=dict(arrowstyle='->', lw=3, color='#E74C3C'))\n",
    "\n",
    "# GPT-3 Box\n",
    "gpt3_box = mpatches.FancyBboxPatch((20.5, 10), 8.5, 7,\n",
    "    boxstyle=mpatches.BoxStyle(\"Round\", pad=0.3),\n",
    "    facecolor='#8E44AD', edgecolor='white', linewidth=2, alpha=0.9)\n",
    "ax.add_patch(gpt3_box)\n",
    "ax.text(24.75, 16.2, 'GPT-3 (May 2020)', fontsize=14, fontweight='bold', ha='center', color='white')\n",
    "ax.text(24.75, 15.2, '175B params (117x)', fontsize=11, ha='center', color='white')\n",
    "gpt3_features = [\n",
    "    'â˜… Alternating Dense+Sparse Attn',\n",
    "    'â€¢ 96 layers, d=12288, 96 heads',\n",
    "    'â€¢ Context: 1024 â†’ 2048 tokens',\n",
    "    'â€¢ 5 datasets (~570GB, 300B tok)',\n",
    "    'â˜… Few-shot In-Context Learning',\n",
    "    'â˜… Emergent abilities at scale'\n",
    "]\n",
    "for i, feat in enumerate(gpt3_features):\n",
    "    color = '#FFEB3B' if feat.startswith('â˜…') else 'white'\n",
    "    ax.text(21, 14.2 - i * 0.7, feat, fontsize=9, color=color, family='monospace')\n",
    "\n",
    "# í•˜ë‹¨ ë²”ë¡€\n",
    "legend_box = mpatches.FancyBboxPatch((5, 1), 20, 3.5,\n",
    "    boxstyle=mpatches.BoxStyle(\"Round\", pad=0.3),\n",
    "    facecolor='#ECF0F1', edgecolor='#BDC3C7', linewidth=1, alpha=0.9)\n",
    "ax.add_patch(legend_box)\n",
    "ax.text(15, 4, 'Key Paradigm Shifts (â˜… = New innovation)', fontsize=12, fontweight='bold', ha='center')\n",
    "shifts = [\n",
    "    'GPT-1â†’2: Post-Norm â†’ Pre-Norm  |  Fine-tune â†’ Zero-shot  |  1/âˆšN weight scaling',\n",
    "    'GPT-2â†’3: Dense â†’ Alternating Sparse Attention  |  Zero-shot â†’ Few-shot In-Context Learning'\n",
    "]\n",
    "for i, s in enumerate(shifts):\n",
    "    ax.text(15, 2.8 - i * 1.0, s, fontsize=10, ha='center', family='monospace', color='#2C3E50')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 8: ìµœì¢… ì •ë¦¬ í€´ì¦ˆ\n",
    "\n",
    "ì•„ë˜ ì§ˆë¬¸ë“¤ì— ë‹µí•˜ë©° ì´ë²ˆ ë©ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ì •ë¦¬í•´ë´…ì‹œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ìµœì¢… í€´ì¦ˆ\n",
    "# ============================================================\n",
    "\n",
    "quiz_questions = [\n",
    "    {\n",
    "        'q': 'Q1. GPT-2ì—ì„œ ë„ì…ëœ Pre-Normì˜ ìˆ˜ì‹ì€? (Post-Normê³¼ ë¹„êµí•˜ì—¬)',\n",
    "        'choices': [\n",
    "            'A) Output = LayerNorm(x + SubLayer(x))',\n",
    "            'B) Output = x + SubLayer(LayerNorm(x))',\n",
    "            'C) Output = LayerNorm(x) + SubLayer(x)',\n",
    "            'D) Output = SubLayer(x + LayerNorm(x))'\n",
    "        ],\n",
    "        'answer': 'B'\n",
    "    },\n",
    "    {\n",
    "        'q': 'Q2. GPT-2ì—ì„œ residual layerì˜ weightë¥¼ ì´ˆê¸°í™”í•  ë•Œ ì‚¬ìš©í•˜ëŠ” scaling factorëŠ”?',\n",
    "        'choices': [\n",
    "            'A) 1/N (N = number of layers)',\n",
    "            'B) 1/âˆšN (N = number of residual connections)',\n",
    "            'C) 1/âˆšd_model',\n",
    "            'D) 1/n_heads'\n",
    "        ],\n",
    "        'answer': 'B'\n",
    "    },\n",
    "    {\n",
    "        'q': 'Q3. GPT-3ì˜ attention êµ¬ì¡°ì—ì„œ ìƒˆë¡­ê²Œ ë„ì…ëœ ê²ƒì€?',\n",
    "        'choices': [\n",
    "            'A) Multi-head attention',\n",
    "            'B) Cross-attention',\n",
    "            'C) Alternating dense and locally-banded sparse attention',\n",
    "            'D) Grouped Query Attention (GQA)'\n",
    "        ],\n",
    "        'answer': 'C'\n",
    "    },\n",
    "    {\n",
    "        'q': 'Q4. GPT ì‹œë¦¬ì¦ˆì˜ í•™ìŠµ íŒ¨ëŸ¬ë‹¤ì„ ë³€í™” ìˆœì„œëŠ”?',\n",
    "        'choices': [\n",
    "            'A) Zero-shot â†’ Fine-tune â†’ Few-shot',\n",
    "            'B) Fine-tune â†’ Few-shot â†’ Zero-shot',\n",
    "            'C) Fine-tune â†’ Zero-shot â†’ Few-shot (In-context Learning)',\n",
    "            'D) Few-shot â†’ Zero-shot â†’ Fine-tune'\n",
    "        ],\n",
    "        'answer': 'C'\n",
    "    },\n",
    "    {\n",
    "        'q': 'Q5. GPT-3ì˜ í•™ìŠµ ë°ì´í„°ì—ì„œ ê°€ì¥ í° ë¹„ìœ¨ì„ ì°¨ì§€í•˜ëŠ” ë°ì´í„°ì…‹ê³¼ ê·¸ í•™ìŠµ ê°€ì¤‘ì¹˜ëŠ”?',\n",
    "        'choices': [\n",
    "            'A) Wikipedia, 60%',\n",
    "            'B) Common Crawl, 60%',\n",
    "            'C) WebText2, 60%',\n",
    "            'D) Books1+Books2, 60%'\n",
    "        ],\n",
    "        'answer': 'B'\n",
    "    },\n",
    "    {\n",
    "        'q': 'Q6. Pre-Normì´ Post-Normë³´ë‹¤ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµì— ìœ ë¦¬í•œ ì´ìœ ëŠ”?',\n",
    "        'choices': [\n",
    "            'A) íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ì¤„ì–´ë“¤ê¸° ë•Œë¬¸',\n",
    "            'B) í•™ìŠµ ì†ë„ê°€ ë¹¨ë¼ì§€ê¸° ë•Œë¬¸',\n",
    "            'C) Gradient flowê°€ ì•ˆì •í™”ë˜ì–´ vanishing/exploding gradientê°€ ì™„í™”ë˜ê¸° ë•Œë¬¸',\n",
    "            'D) Attention scoreê°€ ë” ì •í™•í•´ì§€ê¸° ë•Œë¬¸'\n",
    "        ],\n",
    "        'answer': 'C'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"ğŸ“ ìµœì¢… ì •ë¦¬ í€´ì¦ˆ\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "score = 0\n",
    "for i, quiz in enumerate(quiz_questions):\n",
    "    print(f\"\\n{quiz['q']}\")\n",
    "    for c in quiz['choices']:\n",
    "        print(f\"  {c}\")\n",
    "\n",
    "    # ìë™ ì±„ì  (í•™ìƒì´ ì§ì ‘ ë‹µí•  ìˆ˜ ìˆë„ë¡ ìˆ˜ì • ê°€ëŠ¥)\n",
    "    print(f\"  âœ… ì •ë‹µ: {quiz['answer']}\")\n",
    "    score += 1\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"ì ìˆ˜: {score}/{len(quiz_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 9: ì¢…í•© ìš”ì•½ (Summary)\n",
    "\n",
    "### GPT-1 â†’ GPT-2: ì•„í‚¤í…ì²˜ í˜ì‹ \n",
    "\n",
    "| ë³€ê²½ì‚¬í•­ | ê¸°ìˆ ì  ë‚´ìš© | íš¨ê³¼ |\n",
    "|---------|-----------|------|\n",
    "| **Pre-Norm** | LayerNormì„ sublayer ì…ë ¥ ì „ìœ¼ë¡œ ì´ë™ | Gradient ì•ˆì •í™” â†’ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥ |\n",
    "| **Weight Scaling** | Residual projectionì„ $1/\\sqrt{N}$ìœ¼ë¡œ ì´ˆê¸°í™” | Activation variance í­ë°œ ë°©ì§€ |\n",
    "| **Final LayerNorm** | ë§ˆì§€ë§‰ attention block ë’¤ì— ì¶”ê°€ LayerNorm | ì¶œë ¥ ì•ˆì •í™” |\n",
    "| **Byte-level BPE** | í† í¬ë‚˜ì´ì €ë¥¼ Byte-level BPEë¡œ ë³€ê²½ | OOV ë¬¸ì œ í•´ê²°, 50,257 vocab |\n",
    "| **Paradigm shift** | Fine-tune â†’ Zero-shot | Task-specific í•™ìŠµ ë¶ˆí•„ìš” |\n",
    "\n",
    "### GPT-2 â†’ GPT-3: ìŠ¤ì¼€ì¼ë§ + íŒ¨ëŸ¬ë‹¤ì„ í˜ì‹ \n",
    "\n",
    "| ë³€ê²½ì‚¬í•­ | ê¸°ìˆ ì  ë‚´ìš© | íš¨ê³¼ |\n",
    "|---------|-----------|------|\n",
    "| **Sparse Attention** | Denseì™€ locally-banded sparseë¥¼ êµëŒ€ ì‚¬ìš© | 96 layersì—ì„œì˜ ê³„ì‚° íš¨ìœ¨ í–¥ìƒ |\n",
    "| **Massive Scaling** | 117M â†’ 175B (1,500x) | Emergent abilities ë°œí˜„ |\n",
    "| **In-Context Learning** | Promptì— ì˜ˆì‹œë¥¼ í¬í•¨í•˜ì—¬ task ìˆ˜í–‰ | Fine-tuning ì—†ì´ ë‹¤ì–‘í•œ task ìˆ˜í–‰ |\n",
    "| **Data Diversity** | 5ê°œ ì†ŒìŠ¤ì˜ ê°€ì¤‘ ìƒ˜í”Œë§ | ê³ í’ˆì§ˆ ë°ì´í„° ìš°ì„  í•™ìŠµ |\n",
    "\n",
    "### í•µì‹¬ êµí›ˆ\n",
    "\n",
    "1. **Scale matters, but architecture matters too**: ë‹¨ìˆœíˆ í¬ê¸°ë§Œ í‚¤ìš´ ê²ƒì´ ì•„ë‹ˆë¼ Pre-Norm, weight scaling ë“±ì˜ ì•„í‚¤í…ì²˜ ë³€ê²½ì´ ëŒ€ê·œëª¨ í•™ìŠµì„ ê°€ëŠ¥í•˜ê²Œ í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "2. **Emergent abilities**: ì¶©ë¶„í•œ ìŠ¤ì¼€ì¼ì—ì„œ Fine-tuning ì—†ì´ë„ taskë¥¼ ìˆ˜í–‰í•˜ëŠ” ëŠ¥ë ¥(in-context learning)ì´ \"ì°½ë°œ\"ì ìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.\n",
    "\n",
    "3. **Data quality > quantity**: GPT-3ëŠ” ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ë” ìì£¼ ë°˜ë³µí•˜ê³ , ì €í’ˆì§ˆ ëŒ€ê·œëª¨ ë°ì´í„°ëŠ” ë‹¤ìš´ìƒ˜í”Œë§í•˜ëŠ” ì „ëµì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“š ì°¸ê³  ë¬¸í—Œ (References)\n",
    "\n",
    "1. Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). *Improving Language Understanding by Generative Pre-Training*. OpenAI.\n",
    "\n",
    "2. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). *Language Models are Unsupervised Multitask Learners*. OpenAI.\n",
    "\n",
    "3. Brown, T. B., et al. (2020). *Language Models are Few-Shot Learners*. NeurIPS 2020. arXiv:2005.14165.\n",
    "\n",
    "4. Vaswani, A., et al. (2017). *Attention Is All You Need*. NeurIPS 2017. arXiv:1706.03762.\n",
    "\n",
    "5. Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). *Generating Long Sequences with Sparse Transformers*. arXiv:1904.10509.\n",
    "\n",
    "6. Xiong, R., et al. (2020). *On Layer Normalization in the Transformer Architecture*. ICML 2020. arXiv:2002.04745. (Pre-Normì˜ ì´ë¡ ì  ë¶„ì„)"
   ]
  }
 ]
}
